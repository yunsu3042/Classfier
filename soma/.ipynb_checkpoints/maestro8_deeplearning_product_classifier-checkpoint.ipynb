{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 기반 상품 카테고리 자동 분류 서버 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 학습 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_list = []\n",
    "y_text_list = []\n",
    "enc = sys.getdefaultencoding()\n",
    "with open(\"refined_category_dataset.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "#         print (line)\n",
    "        info = json.loads(line.strip())\n",
    "        x_text_list.append((info['pid'],info['name']))\n",
    "        y_text_list.append(info['cate'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(y_name_id_dict,\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형식으로 되어 있는 카테고리 명을 숫자 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_name_id_dict = joblib.load(\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'취미': 5, '뷰티': 0, '출산/육아': 16, '반려동물': 4, '디지털': 10, '도서/문구': 8, '스포츠/레저': 9, '잡화': 14, '건강': 7, '생필품/주방': 13, '의류': 2, '자동차/공구': 1, '컴퓨터': 3, '가구/인테리어': 11, '가전': 12, '식품': 6, '여행/e쿠폰': 15}\n"
     ]
    }
   ],
   "source": [
    "print(y_name_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# y_name_set = set(y_text_list)\n",
    "# y_name_id_dict = dict(zip(y_name_set, range(len(y_name_set))))\n",
    "# print(y_name_id_dict.items())\n",
    "# y_id_name_dict = dict(zip(range(len(y_name_set)),y_name_set))\n",
    "y_list = [y_name_id_dict[x] for x in y_text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test 분리하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 기반 text 분류에 필요한 모듈 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import gensim\n",
    "import keras.preprocessing.text\n",
    "import numpy\n",
    "import gensim\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파일을 만약 만들었다면, 아래와 같이 로드 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('/workspace/resources/11st_all_product_name.segment.0918.15w100e3min.model', binary=True)\n",
    "# word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 데이터를 word-id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-04f1743c776d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msequence_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msequence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "sequence_tokenizer.fit_on_texts(X_train)\n",
    "max_features = len(sequence_tokenizer.word_index)\n",
    "\n",
    "\n",
    "def texts_to_sequences2(d_list, tokenizer, maxlen=300):\n",
    "    seq = tokenizer.texts_to_sequences(d_list)\n",
    "    print('mean:', numpy.mean([len(x) for x in seq]))\n",
    "    print('std:', numpy.std([len(x) for x in seq]))\n",
    "    print('median:', numpy.median([len(x) for x in seq]))\n",
    "    print('max:', numpy.max([len(x) for x in seq]))\n",
    "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = texts_to_sequences2(map(lambda i : i[1],X_train),sequence_tokenizer)\n",
    "test = texts_to_sequences2(map(lambda i : i[1],X_test),sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word의 embedding 형태의 weight 를 초기화 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = train.shape[1]\n",
    "\n",
    "input_tensor = keras.layers.Input(shape=(input_dim,), dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vec_dim = 100\n",
    "not_ct = 0\n",
    "weights = numpy.zeros((max_features + 1, word_vec_dim))\n",
    "for word, index in sequence_tokenizer.word_index.items():\n",
    "    if False:\n",
    "        pass\n",
    "#     if word in word2vec.vocab:\n",
    "#         weights[index, :] = word2vec[word]\n",
    "    else:\n",
    "        not_ct+=1\n",
    "        weights[index, :] = numpy.random.uniform(-0.25, 0.25, word_vec_dim)\n",
    "# del word2vec\n",
    "# del sequence_tokenizer\n",
    "print (not_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  학습할 레이러를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded = keras.layers.Embedding(input_dim=max_features + 1,\n",
    "                                  output_dim=word_vec_dim, input_length=input_dim,\n",
    "                                  weights=[weights],trainable=True)(input_tensor)\n",
    "# embedded2 = keras.layers.Dropout(0.9)(embedded)\n",
    "# embedded2 = embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensors = []\n",
    "for filter_length in [3, 5]:\n",
    "    tensor = keras.layers.Convolution1D(nb_filter=50, filter_length=filter_length)(embedded)\n",
    "    tensor = keras.layers.Activation('relu')(tensor)\n",
    "    tensor = keras.layers.MaxPooling1D(pool_length=input_dim - filter_length + 1)(tensor)\n",
    "    tensor = keras.layers.Flatten()(tensor)\n",
    "    tensors.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embedded = keras.layers.Dropout(0.5)(embedded)\n",
    "output_tensor = keras.layers.merge(tensors, mode='concat', concat_axis=1)\n",
    "# output_tensor = keras.layers.Dropout(0.5)(output_tensor) # 0.7312\n",
    "output_tensor = keras.layers.Dropout(0.5)(output_tensor) \n",
    "output_tensor = keras.layers.Dense(len(set(y_list)), activation='softmax')(output_tensor)\n",
    "\n",
    "# output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn) # See equations (6) and (7).\n",
    "\n",
    "cnn = keras.models.Model(input_tensor, output_tensor)\n",
    "cnn.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn.fit(train, np.asarray(to_categorical(y_train)), batch_size=60, nb_epoch=10,\n",
    "        validation_data=(test, np.asarray(to_categorical(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_x_text_list = []\n",
    "with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        eval_x_text_list.append((info['pid'],info['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "pred = cnn.predict(eval_x_list)\n",
    "pred_list = [argmax(y) for y in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "name='test1'\n",
    "nickname='test1nickname'\n",
    "mode='test'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list)),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 으로 추출한 이미지 데이터 사용하기 \n",
    " * 이 부분은 각자 한번 해보도록 해요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
