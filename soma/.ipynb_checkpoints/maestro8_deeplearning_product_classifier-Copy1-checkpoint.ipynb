{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 기반 상품 카테고리 자동 분류 서버 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 학습 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_list = []\n",
    "y_text_list = []\n",
    "enc = sys.getdefaultencoding()\n",
    "with open(\"refined_category_dataset.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "#         print (line)\n",
    "        info = json.loads(line.strip())\n",
    "        x_text_list.append((info['pid'],info['name']))\n",
    "        y_text_list.append(info['cate'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(y_name_id_dict,\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형식으로 되어 있는 카테고리 명을 숫자 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_name_id_dict = joblib.load(\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'취미': 5, '뷰티': 0, '출산/육아': 16, '반려동물': 4, '디지털': 10, '도서/문구': 8, '스포츠/레저': 9, '잡화': 14, '건강': 7, '생필품/주방': 13, '의류': 2, '자동차/공구': 1, '컴퓨터': 3, '가구/인테리어': 11, '가전': 12, '식품': 6, '여행/e쿠폰': 15}\n"
     ]
    }
   ],
   "source": [
    "print(y_name_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# y_name_set = set(y_text_list)\n",
    "# y_name_id_dict = dict(zip(y_name_set, range(len(y_name_set))))\n",
    "# print(y_name_id_dict.items())\n",
    "# y_id_name_dict = dict(zip(range(len(y_name_set)),y_name_set))\n",
    "y_list = [y_name_id_dict[x] for x in y_text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test 분리하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 기반 text 분류에 필요한 모듈 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import keras.preprocessing.text\n",
    "import numpy\n",
    "import gensim\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파일을 만약 만들었다면, 아래와 같이 로드 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('/workspace/resources/11st_all_product_name.segment.0918.15w100e3min.model', binary=True)\n",
    "# word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 데이터를 word-id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1284373589', '도그차일드 레오파드애견신발 노랑 강아지슈즈 애'),\n",
       " ('1603361243', '불스 방청윤활제 360ML 윤활방청제 방청유 윤활유 녹'),\n",
       " ('1086777272', '[쿠스쿠스파이]블루베리치즈파이 800g(100g x 8조각)'),\n",
       " ('1367029764', '파사바체 유리 계량컵/주방용품/계량컵/계량용컵/조리도구/계량용품'),\n",
       " ('1687461732',\n",
       "  '제주 코코몽에코파크+메이즈랜드  제주도 관광지 2곳 패키지 입장권 할인 /기프트제주/제주 승마체험/제주 승마/제주승마할인/제주 승마장 추천/제주도 승마체험 가격말타는곳 말타기'),\n",
       " ('1639169754', 'BA537 발목 서포트 LP-954 압박밴드 - 발목보호대 무'),\n",
       " ('1470415954', '[중고]서버용 HDD SAS Seagate ST300MP0005 300GB/15K'),\n",
       " ('1255743159', '(투핫) [헬로키티]헬로키티 쿠킹컵 세트'),\n",
       " ('1611544124', '인테리어 스위치커버 그래픽스티커 파인애플 WBSS7170'),\n",
       " ('1486501072', '멋스러운 캐주얼 정장 서스펜더 멜빵 TE1005'),\n",
       " ('1549175351', '1200M [미코아이엔티] 아트조이 DIY 명화그리기 숲속의 작은집'),\n",
       " ('1401228966', '3M  PF 프라이버시 필터 (15.6W)'),\n",
       " ('1530257201', 'LD-PA5 인도 사우디아라비아 등 해외여행 아답타'),\n",
       " ('1321872342', '[착한스포츠]STAR 고무연식 야구공(12개입)'),\n",
       " ('1578581418', '다람쥐PMC-230 고양이장난감 고양이용장난감 캣장난감'),\n",
       " ('1036567080',\n",
       "  'PS무료배송 [랄라룹시 윈터드레스패션세트(506539)] (인형별매) 랄라룹시 윈터드레스 패션세트 인형옷 인..'),\n",
       " ('1651852477', '보브 듀얼 커버 CC크림 30ml 21호 라이트 톤업 커버'),\n",
       " ('1629587718', '허니버터링쿠키(요쿠르트)국내산쿠키 X2개 강아지과자'),\n",
       " ('1589969317', '태성 캐논정품후드 EW-54 (EF-M 18-55mm 전용 후드)'),\n",
       " ('1624338116', '가을!! 날라리 단가라 상하 -BK /16FHONEY016'),\n",
       " ('1630907693', '아둘 패밀리룩 단가라 니트 기모 가족티 스트라이프76'),\n",
       " ('1333678802', 'Casio 카시오키보드 SA-46/32건반/미니건반/키보드/전자키보드/SA46 (SA-46)'),\n",
       " ('1536088', '참숯 옥황토 맥반석 찜질팩 DS3860H 타이머 뜸질기'),\n",
       " ('1633724233', '종이시디케이스(100매 CD보관 종이씨디케이스 CD수납'),\n",
       " ('1635747214', '코럴B화병A 인테리어화병 인테리어소품화병 디자인꽃'),\n",
       " ('898479883', '[포그니 성인용 소변패드] 국산/천연펄프/일자/대형/프리미엄/강력흡수/환자/간병/노인/병원/기저귀/팬티'),\n",
       " ('1634179330', '볼마커 시즌4(19-25) 볼클립 골프용품 캐릭터볼마커'),\n",
       " ('1590856403', 'B15219/파워레인저 닌자포스 스페이스건 18000/장난감'),\n",
       " ('1501675929', '(3111230) 유니볼 시그노 이레이져블 볼펜 12 1갑'),\n",
       " ('1532766880', '기노 맹세르 라피드 250ml/샘플램덤배송'),\n",
       " ('1524382321', 'LG생활건강 여행용 프리미엄세트 세면도구7종 / 여행세면도구 여행용샴푸 여행용칫솔 여행용세안도구 휴대'),\n",
       " ('471118872', '유유 메가파워 루테인 2병/4개월 눈건강 5중복합기능'),\n",
       " ('1241243790', '삼성정품 진공 청소기 필터(헤파)/사용모델:VC-MCN704'),\n",
       " ('1333492734', '[엑스포넷]신체보호대 손목보호대 손보호대 보호대 보호밴드 손목밴드 손밴드 손목랩 손랩 손목보호대 손목'),\n",
       " ('1650611273', '2017년 구정선물세트 기분좋은 6호 LG생활건강 샴푸선'),\n",
       " ('1428108377', 'Fantastic  OCEAN PARK   홍콩 오션파크 일급  3박4일'),\n",
       " ('1095866798', '보령 홈라이프 매트에스 60매 x 3박스 //홈매트'),\n",
       " ('990624169', '개구마르의진화 도치마론의진화 푸호꼬의진화 / 포켓몬 카드 XY 퍼스트세트'),\n",
       " ('1000142355', '메요스탠드 2발 MFS-MS500  / 병원용품/운반카/드레싱카트   hjin'),\n",
       " ('1345044714', '토니(블루) 대방석세트(솜포함) 40수 순면'),\n",
       " ('1383449266', '쌍마 코일호스 PE 8x5x5M/호스/호스릴'),\n",
       " ('1472853264', '[태국/파타야 호텔예약] 라리나 리조트(Koh Lan/Lareena Resort)호텔검색,호텔가격'),\n",
       " ('1373917807', '모건블루 morganblue 바이크워시 자전거체인오일 체인오일 자전거구리스 더러워진손 세척제'),\n",
       " ('1470363323', '[삼천리] 2016 씨티크루저M26   생활용 MTB 자전거 26형 21단'),\n",
       " ('1465755464', '보여도 좋은 레이스 속바지 레이어드 플라워'),\n",
       " ('1625212930', '프로마크 MEMO ACEVEDO 팀발레스 스틱 MA1'),\n",
       " ('1460718764', '아이앤루디800컴퓨터책상세트TGW'),\n",
       " ('1646562587', '[디씨팡]/해피바스 로맨틱 무스크 샤워코롱 150ml 바'),\n",
       " ('1613577666', '3M보안경/자전거고글/보안경/보호안경/고글/모음/2750'),\n",
       " ('1484088162', '쌍용C&B 코디 후레시아 황사마스크 KF80 x 10매'),\n",
       " ('1631066379', '당일출고 미니돌핀 네블라이저 돌고래'),\n",
       " ('1679775947',\n",
       "  '[아시아패스]방콕 파타야출발 /꼬사메산 + 파타야 원데이 / 농눅빌리지/파타야수상시장/파타야일일투어/파타야자유여행'),\n",
       " ('1483381543', '[리얼룩] 소니 워크맨 NWZ-A15 투명 액정보호필름'),\n",
       " ('1457016194', '화이트 보드 2개 묶음 칠판 판 이동식 미니 자석 유아 게시판 알림판'),\n",
       " ('1661026488', 'HP 15-AC624TXS보안필름/거치형'),\n",
       " ('1607102583', '마몽드 에이지 컨트롤 파워 아이크림 (25ml)'),\n",
       " ('1648563193', '수공제작 14K gold 천연다이아 최고급 로즈 2종 세트'),\n",
       " ('1377897636', '새일 컷트가위 KSC5500'),\n",
       " ('1570345912', 'K3 장기렌트카 자동차리스 기아차리스 장기렌트비용'),\n",
       " ('1462071872', '1300K [아쿠아리스트] JBL 크리니 외부여과기 호스청소솔 CLEANY 호스전용'),\n",
       " ('684865132', '소포장 훈제연어 슬라이스 150g 200g 300g'),\n",
       " ('1470329271', '무료 위드앤올 Vstarcam-100E 130E 네트워크IP카메라['),\n",
       " ('1392578475', 'JJ-00431 양양 슬립온'),\n",
       " ('1559602567', '고급 클래식 체스판'),\n",
       " ('1619996401', '무료배송.사랑스런 전체 레이스 프릴블라우스/면/지니'),\n",
       " ('1608730909', '파일 화일 플라스틱 아크릴 A4 사무용 클립보드'),\n",
       " ('1517010317',\n",
       "  'B15034/레전드히어로 삼국전 태엽로봇세트 15000/태엽을감으면 아장아장걷는 뽀로로 루피 에디 크롱 패티'),\n",
       " ('1618026166', '캣 워터에이디티브(구취제거) (약453ml) 강아지용품'),\n",
       " ('1665746050', '메츠 Metz Mecablitz 44 AF-2 Digital 플래시 (올림푸'),\n",
       " ('1446243556', '비행기조립 교구(미니 제트기)공항 /장난감비행기/모형비행기/공항놀이/미니어처/유아동비행기'),\n",
       " ('1675422644', '자바 POP 12색 사각매직'),\n",
       " ('1646222770', '과산화수소 4L 메디탑 살균소독제/탈취/표백/산화'),\n",
       " ('1597939516', '[중고]컬럼비아 주니어 춘추 체크 면 남방/ XL'),\n",
       " ('1503495398', '만능유리창닦이 180도회전 청소용품 극세사걸레'),\n",
       " ('1713882505', '깜놀까  괌 니코 슈페리어 오션뷰룸 5일'),\n",
       " ('1648056066', 'ATEN / CS82U / 2포트 PS2,USB / KVM 스위치'),\n",
       " ('1469268457', '버너케이스 캠핑 케이스 수납가방 -팀버라인'),\n",
       " ('1487008662',\n",
       "  '[Q2 Power] World to  Italy with USB/이태리 에서 사용가능 /모바일 제품 충전가능'),\n",
       " ('1557108984', '제스파/옥 힐링 지압슬리퍼/발지압/신발/발바닥/건강/'),\n",
       " ('1628257941', '2공펀치(503 평화)'),\n",
       " ('1386850303', '벡셀 BEXEL - 반디 후미등 랜턴 - 자전거 전조등/후미등/안전등/랜턴/LED'),\n",
       " ('1628278139', 'q 바른자세 릴렉스백 등/허리 스트레칭 /허리운동'),\n",
       " ('1661572769',\n",
       "  '제주 다빈치뮤지엄+유리의성&마법의숲 제주도 관광지 2곳 패키지 입장권 할인 /기프트제주/제주 승마체험/제주 승마/제주승마할인/제주 승마장 추천/제주도 승마체험 가격말타는곳 말타기'),\n",
       " ('1586340950', '[치코본사직영]라이트웨이 유모차-그린'),\n",
       " ('1409332684', '[분당종합피아노] 카시오 디지털피아노 CDP-230R'),\n",
       " ('1515358670',\n",
       "  '2015년 방문 바람막이(90X210cm) 방풍비닐 창문 커튼 뽁뽁이 단열 자석 자동 문풍지 방풍비닐 바람막이 방'),\n",
       " ('1708914003', '[GRET](세부)(에코)포시즌 마사지 140분'),\n",
       " ('1538873662', '[핫트랙스] 체리 - [포장지훼손 특가] 스위트 미러 손거울'),\n",
       " ('1583555798', '오로니아 오메가 꾸미 츄어블 120꾸미 x 2개'),\n",
       " ('1041277949', '[비젼몰]안전잠금이중형/LP-34-1/서랍잠금장치/유아안전용품'),\n",
       " ('514045742',\n",
       "  '@MD강력추천정품@웰빙요리/그릴/쿠커/전기그릴/미니오븐/로쏘울프/요리도구/홈베이킹/컨벡션/광파오븐콤보 고급형/'),\n",
       " ('1041072677', '[올림푸스 레이저드릴검사현미경]'),\n",
       " ('1686772595', ')플러스펜S(0.4mm_블랙)_다스(12개입) 수성펜 필기도'),\n",
       " ('1661387334', 'VR TIME 가상현실 체험기기'),\n",
       " ('1372125914',\n",
       "  '[GL] 후지제록스 CT350673/정품/노랑/표준용량/C2200/DocuPrint-C2200 C3300DX/C3300DX/CT350671/CT350672/CT350675/CT35'),\n",
       " ('1671662434', '리케이 숱가위 40방[V-1540]/애견용품'),\n",
       " ('1384369600', '[에뜨와]비타민욕조[핑크] (77F0 86001)'),\n",
       " ('1555394845', '남성명함지갑 심플한 명함케이스 AT82006A 사무용품'),\n",
       " ('1218313863',\n",
       "  '[AKMUSIC]KAWAI 가와이 디지털피아노 CN-34 / CN34 [정품 블랙/로즈우드/화이트]전화문의시 최저가 안내!!!'),\n",
       " ('1542269096', '파피야 멀티 클리퍼 AC3701P 애견위생용품 애견용품')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x10b914c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "sequence_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.preprocessing.text.Tokenizer at 0x10b914f60>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_tokenizer = tf.contrib.keras.preprocessing.text.Tokenizer()\n",
    "sequence_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-80f8329c7732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/tensorflow/contrib/keras/python/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       seq = text if self.char_level else text_to_word_sequence(\n\u001b[0;32m--> 161\u001b[0;31m           text, self.filters, self.lower, self.split)\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/tensorflow/contrib/keras/python/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \"\"\"\n\u001b[1;32m     53\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "sequence_tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-04f1743c776d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msequence_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msequence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "sequence_tokenizer.fit_on_texts(X_train)\n",
    "max_features = len(sequence_tokenizer.word_index)\n",
    "\n",
    "\n",
    "def texts_to_sequences2(d_list, tokenizer, maxlen=300):\n",
    "    seq = tokenizer.texts_to_sequences(d_list)\n",
    "    print('mean:', numpy.mean([len(x) for x in seq]))\n",
    "    print('std:', numpy.std([len(x) for x in seq]))\n",
    "    print('median:', numpy.median([len(x) for x in seq]))\n",
    "    print('max:', numpy.max([len(x) for x in seq]))\n",
    "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = texts_to_sequences2(map(lambda i : i[1],X_train),sequence_tokenizer)\n",
    "test = texts_to_sequences2(map(lambda i : i[1],X_test),sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word의 embedding 형태의 weight 를 초기화 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = train.shape[1]\n",
    "\n",
    "input_tensor = keras.layers.Input(shape=(input_dim,), dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vec_dim = 100\n",
    "not_ct = 0\n",
    "weights = numpy.zeros((max_features + 1, word_vec_dim))\n",
    "for word, index in sequence_tokenizer.word_index.items():\n",
    "    if False:\n",
    "        pass\n",
    "#     if word in word2vec.vocab:\n",
    "#         weights[index, :] = word2vec[word]\n",
    "    else:\n",
    "        not_ct+=1\n",
    "        weights[index, :] = numpy.random.uniform(-0.25, 0.25, word_vec_dim)\n",
    "# del word2vec\n",
    "# del sequence_tokenizer\n",
    "print (not_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  학습할 레이러를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded = keras.layers.Embedding(input_dim=max_features + 1,\n",
    "                                  output_dim=word_vec_dim, input_length=input_dim,\n",
    "                                  weights=[weights],trainable=True)(input_tensor)\n",
    "# embedded2 = keras.layers.Dropout(0.9)(embedded)\n",
    "# embedded2 = embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensors = []\n",
    "for filter_length in [3, 5]:\n",
    "    tensor = keras.layers.Convolution1D(nb_filter=50, filter_length=filter_length)(embedded)\n",
    "    tensor = keras.layers.Activation('relu')(tensor)\n",
    "    tensor = keras.layers.MaxPooling1D(pool_length=input_dim - filter_length + 1)(tensor)\n",
    "    tensor = keras.layers.Flatten()(tensor)\n",
    "    tensors.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embedded = keras.layers.Dropout(0.5)(embedded)\n",
    "output_tensor = keras.layers.merge(tensors, mode='concat', concat_axis=1)\n",
    "# output_tensor = keras.layers.Dropout(0.5)(output_tensor) # 0.7312\n",
    "output_tensor = keras.layers.Dropout(0.5)(output_tensor) \n",
    "output_tensor = keras.layers.Dense(len(set(y_list)), activation='softmax')(output_tensor)\n",
    "\n",
    "# output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn) # See equations (6) and (7).\n",
    "\n",
    "cnn = keras.models.Model(input_tensor, output_tensor)\n",
    "cnn.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn.fit(train, np.asarray(to_categorical(y_train)), batch_size=60, nb_epoch=10,\n",
    "        validation_data=(test, np.asarray(to_categorical(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_x_text_list = []\n",
    "with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        eval_x_text_list.append((info['pid'],info['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "pred = cnn.predict(eval_x_list)\n",
    "pred_list = [argmax(y) for y in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "name='test1'\n",
    "nickname='test1nickname'\n",
    "mode='test'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list)),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 으로 추출한 이미지 데이터 사용하기 \n",
    " * 이 부분은 각자 한번 해보도록 해요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
