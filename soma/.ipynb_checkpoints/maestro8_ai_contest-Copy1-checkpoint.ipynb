{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8기 과제 - 딥러닝 기반 상품 카테고리 자동 분류 서버"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 개요\n",
    "* 출제자 : 남상협 멘토 (justin@buzzni.com) / 버즈니 (http://buzzni.com) 대표\n",
    "* 배경 : 카테고리 분류 엔진은 실제로 많은 서비스에서 사용되는 중요한 기계학습 기술이다. 본 과제의 주제는 버즈니 개발 인턴이자 마에스트로 6기 멘티가 아래와 나와 있는 기본 분류 모델을 기반으로 deep learning 기반의 feature 를 더해서 고도화된 분류 엔진을 만들어서 2016 한국 정보과학회 논문으로도 제출 했던 주제이다. 기계학습에 대한 학습과, 실용성 두가지 측면에서 모두 도움이 될 것으로 보인다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 목표\n",
    "* 입력 : 상품명, 상품 이미지\n",
    "* 출력 : 카테고리\n",
    "* 목표 : 가장 높은 정확도로 분류를 하는 분류 엔진을 개발\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 평가 항목 \n",
    "* 성능평가 (100%)\n",
    " \n",
    "## 제출 항목 \n",
    "* 채점 서버에 자신이 분류한 class id 리스트를 파라미터로 넣어서 호출한다. \n",
    "* name - 자신의 이름을 넣는다. 실제 점수판에는 공개가 안됨, 추후 평가시에 일치하는 이름의 멘티 점수로 사용함. 요청한 평가 중에서 가장 높은 점수의 평가 점수로 업데이트됨.\n",
    "* nickname - 점수판에 공개되는 이름, 자신의 이름으로 해도 되고, 닉네임으로 해도 됨. 구분을 위해서 사용하는 feature(text, textimage) 와 알고리즘 (svm, cnn) 등을 닉네임 뒤에 붙여준다. \n",
    "* pred_list - 분류한 카테고리 id 리스트를 , 로 묶은 데이터 \n",
    "* 평가 점수가 반환된다. - precision, 높을 수록 좋다. 두가지 방법 각각 50%씩 점수 반영 \n",
    "* mode - 'test' 로 호출하면 웹으로 순위가 공개되는 테스트 평가를 수행하고 결과 점수가 반환된다. 해당 결과 점수는 http://eval.buzzni.net:20002/score 에서 확인 가능함. 실제 성적 평가는 'eval' 로 평가용 데이터로 호출하면 된다. 이때는 점수가 반환되거나, 웹 점수 보드에도 나오지 않는다. \n",
    "* 너무 자주 평가를 요청하기 보다, 가급적 자체적으로 평가 해서, 괜찮게 나올때 요청하길 권장 \n",
    "```python\n",
    "import requests\n",
    "name='test1'\n",
    "nickname='test1_text_svm'\n",
    "mode='test' #'eval' 을 실제 성적 평가용. 분류 점수 반환 안됨.\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list)),\n",
    "         'name':name,'nickname':nickname,\"mode\":mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())         \n",
    "         ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 향상 포인트\n",
    "* http://localhost:8000/notebooks/maestro8_deeplearning_product_classifier.ipynb 이 노트북에 있는 딥러닝 기반의 분류기로 분류할 경우에 더 높은 성능을 낼 수 있어서 유리함\n",
    "* 아래의 방법들은 하나의 예이고, 아래에 나와 있지 않은 다양한 방법들도 가능함.\n",
    "* 전처리 \n",
    " * 오픈된 형태소 분석기(예 - konlpy) 를 써서, 단어 띄어쓰기를 의미 단위로 띄어서 학습하기\n",
    " * bigram, unigram, trigram 등 단어 feature 를 더 다양하게 추가하기\n",
    "* 딥러닝 \n",
    " * embedding weight 를 random 이 아닌 학습된 값을 사용하기 (https://radimrehurek.com/gensim/models/word2vec.html)\n",
    " * 이미지 feature 를 CNN으로 추출할때 더 성능이 좋은 모델 사용하기 (예제로 준 데이터는 mobilenet 으로 성능보다 속도 위주로 된 모델)\n",
    " * 다양한 파라미터(hyper parameter) 로 실험 해보기 \n",
    "* 피쳐 조합  \n",
    " * 이미지 feature 와 text feature 를 합치는 부분 잘하기 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 점수 서버 \n",
    "* 현재 평가 순위를 json 형태로 반환한다.\n",
    "* 여러번 호출했을때는 가장 높은 점수로 업데이트 한다.\n",
    " * http://eval.buzzni.net:20002/score\n",
    "* 실제 점수는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma, Twitter\n",
    "from konlpy.corpus import kolaw\n",
    "from konlpy.utils import pprint\n",
    "from nltk import collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunsoo/.pyenv/versions/3.4.3/envs/cnn/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/yunsoo/.pyenv/versions/3.4.3/envs/cnn/lib/python3.4/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.externals import  joblib\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import  LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer,CountVectorizer\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "# from keras import backend\n",
    "# from keras.layers import Dense, Input, Lambda, LSTM, TimeDistributed\n",
    "# from keras.layers.merge import concatenate\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"patched_list_v3\", 'rb') as f:\n",
    "    eng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8a6e6e663214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yunsoo/.pyenv/versions/3.4.3/envs/cnn/lib/python3.4/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[1;32m    234\u001b[0m                         \" or shape[0]\")\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "len(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'가구/인테리어': 11,\n",
       " '가전': 12,\n",
       " '건강': 7,\n",
       " '도서/문구': 8,\n",
       " '디지털': 10,\n",
       " '반려동물': 4,\n",
       " '뷰티': 0,\n",
       " '생필품/주방': 13,\n",
       " '스포츠/레저': 9,\n",
       " '식품': 6,\n",
       " '여행/e쿠폰': 15,\n",
       " '의류': 2,\n",
       " '자동차/공구': 1,\n",
       " '잡화': 14,\n",
       " '출산/육아': 16,\n",
       " '취미': 5,\n",
       " '컴퓨터': 3}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_name_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/yunsoo/mysite-projects/text_cnn_fork/train_eng.kkk\", 'w') as f:\n",
    "    for pid, name in eng:\n",
    "        for idx, data in enumerate(x_text_list):\n",
    "            if pid == data[0]:\n",
    "                for key, val in y_name_id_dict.items():\n",
    "                    if val == y_list[idx]:\n",
    "                        category = key\n",
    "                        f.write( name.replace(',', ' ') + \",\" +category + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('1234621373', '#MD추천정품# 심플베드 엘레강스 고급형 접이식침대 더블사이즈115cm/수동/등받이6단으로 원하는각도조절')\n",
      "1 ('1543513007', '(9731200) 소형제도판 (S) 661 A3')\n",
      "2 ('1546650266', '(AGCRIP 웨빙 포켓조끼 빅사이즈 조끼 MK321_324 아웃')\n",
      "3 ('1589443530', '(ATK아텍스 전동접이식침대(BE556) 전동침대 간병용침')\n",
      "4 ('1520510486', '(MST플래그 메세지 캔들 마더 향초 양초캔들 캔들선물 아로마캔들 향초캔들')\n",
      "5 ('1512822613', '(광일체어) 스카치A형 3인 등무 장의자')\n",
      "6 ('1525985492', '(광일체어) 스파고아우드 2인 등유 로비용장의자')\n",
      "7 ('1326388222', '(라움스튜디오)스크류 블랙봉 (일반-2.5cm) / 블랙 커튼봉/ 25mm / 일반봉 / 커튼봉 / 블랙')\n",
      "8 ('1426607155', '(모슬리메모리즈) 산토리니 브리즈 (중) 모슬리메모리즈 아로마캔들 디퓨저 소이캔들 소이왁스')\n",
      "9 ('1334473563', '(무료배송)양키캔들 악세사리 윅디퍼 심지가위 라이터')\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(x_text_list[:10]):\n",
    "    print(idx, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[1,2,3].remove(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_noun(datas):\n",
    "    kkma = Kkma()\n",
    "    word_list = []\n",
    "    for data in datas:\n",
    "        line = kkma.nouns(data[1])\n",
    "        for word in line:\n",
    "            try: \n",
    "                int(word)\n",
    "                line.remove(word)\n",
    "            except:\n",
    "                if word in string.punctuation:\n",
    "                    line.remove(word)\n",
    "        line = \" \".join(line)\n",
    "        word_list.append(line)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twitter(datas):\n",
    "    kkma = Twitter()\n",
    "    word_list = []\n",
    "    for data in datas:\n",
    "        line = kkma.nouns(data[1])\n",
    "        line = \" \".join(line)\n",
    "        word_list.append(line)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hop(datas):\n",
    "    kkma = Kkma()\n",
    "    twit = Twitter()\n",
    "    word_list = []\n",
    "    for data in datas:\n",
    "        line = kkma.nouns(data[1])\n",
    "        line2 = twit.nouns(data[1])\n",
    "        line = line + line2\n",
    "        line = \" \".join(line)\n",
    "        word_list.append(line)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-931e0b40cc22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "extract_noun(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dde7f202a082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "hop(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c92fb8befdf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "twitter(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bd8e3b7bcf95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "datas = twitter(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-13b733191756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meng_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0meng_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datas' is not defined"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "eng_list = []\n",
    "for x in datas:\n",
    "    eng_list.append(translator.translate(x).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_text_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0b237f72bc01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_text_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0meng_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_text_list' is not defined"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "for i, data in enumerate(x_text_list) :\n",
    "    eng_list.append(translator.translate(data[1]).text)\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "        with open(\"train_eng{}.pickle\".format(i), 'wb') as f:\n",
    "            pickle.dump(eng_list, f)\n",
    "            eng_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def withInSpecials(strings):\n",
    "    for ch in strings:\n",
    "        if (ch in string.punctuation):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def hop_some(datas):\n",
    "    kkma = Kkma()\n",
    "    twit = Twitter()\n",
    "    word_list = []\n",
    "    for data in datas:\n",
    "        line = kkma.nouns(data[1])\n",
    "        \n",
    "        for x in line:\n",
    "            if x in  string.punctuation:\n",
    "                line.remove()\n",
    "        line = \" \".join(line)\n",
    "        word_list.append(line)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eng.pickle{}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-50e764f8718c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eng.pickle{}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eng.pickle{}'"
     ]
    }
   ],
   "source": [
    "with open(\"eng.pickle{}\", 'rb') as f:\n",
    "    pickle.dump(eng_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = extract_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 학습 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_list = []\n",
    "y_text_list = []\n",
    "enc = sys.getdefaultencoding()\n",
    "with open(\"refined_category_dataset.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "#         print (line)\n",
    "        info = json.loads(line.strip())\n",
    "        x_text_list.append((info['pid'],info['name']))\n",
    "        y_text_list.append(info['cate'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(y_name_id_dict,\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형식으로 되어 있는 카테고리 명을 숫자 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_name_id_dict = joblib.load(\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'반려동물': 4, '출산/육아': 16, '잡화': 14, '도서/문구': 8, '가구/인테리어': 11, '여행/e쿠폰': 15, '식품': 6, '건강': 7, '뷰티': 0, '컴퓨터': 3, '자동차/공구': 1, '스포츠/레저': 9, '생필품/주방': 13, '의류': 2, '디지털': 10, '취미': 5, '가전': 12}\n"
     ]
    }
   ],
   "source": [
    "print(y_name_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# y_name_set = set(y_text_list)\n",
    "# y_name_id_dict = dict(zip(y_name_set, range(len(y_name_set))))\n",
    "# print(y_name_id_dict.items())\n",
    "# y_id_name_dict = dict(zip(range(len(y_name_set)),y_name_set))\n",
    "y_id_list = [y_name_id_dict[x] for x in y_text_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형태로 되어 있는 상품명을 각 단어 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_list = vectorizer.fit_transform(final(x_text_list))\n",
    "y_list = [y_name_id_dict[x] for x in y_text_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test 분리하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1546650266', '(AGCRIP 웨빙 포켓조끼 빅사이즈 조끼 MK321_324 아웃')\n"
     ]
    }
   ],
   "source": [
    "print(x_text_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(vectorizer.transform(list(map(lambda i : i[1],X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 몇개의 파라미터로 간단히 테스트 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for c in [1,5,10]:\n",
    "#     clf = LinearSVC(C=c)\n",
    "#     X_train_text = map(lambda i : i[1],X_train\n",
    "#     clf.fit(vectorizer.transform(X_train_text), y_train)\n",
    "#     print (c,clf.score(vectorizer.transform(map(lambda i : i[1],X_test)), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = extract_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_noun = final(X_train)\n",
    "X_test_noun = final(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(1, 25):\n",
    "#     c = 0.07 + (i / 1000)\n",
    "#     clf = LinearSVC(C=c)\n",
    "#     X_train_text = map(lambda i : i[1],X_train)\n",
    "#     clf.fit(vectorizer.transform(X_train_noun), y_train)\n",
    "#     print (c,clf.score(vectorizer.transform(X_test_noun), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c = 0.081\n",
    "# clf = LinearSVC(C=c)\n",
    "# X_train_text = map(lambda i : i[1],X_train)\n",
    "# clf.fit(vectorizer.transform(X_train_noun), y_train)\n",
    "# print (c,clf.score(vectorizer.transform(X_test_noun), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적의 파라미터를 알아서 다 해보고, n-fold cross validation까지 해주는 방법 - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_nouns = extract_noun(x_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_param = np.logspace(-1,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsvc = GridSearchCV(LinearSVC(), param_grid= {'C': svc_param}, cv = 5, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsvc.fit(vectorizer.transform(x_text_nouns), y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gsvc.best_score_, gsvc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 데이터에 대해서 분류를 한 후에  평가 서버에 분류 결과 전송"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x_text_list = []\n",
    "with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        test_x_text_list.append((info['pid'],info['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],test_x_text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list = gsvc.predict(vectorizer.transform(extract_noun(test_x_text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"prediction.pickle\", 'rb') as f:\n",
    "    pred_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print (pred_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "name='정윤수'\n",
    "nickname='bro_text_cnn'\n",
    "mode='test'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list.tolist())),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "\n",
    "print (d.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eval 데이터에 대해서 분류를 한 후에  평가 서버에 분류 결과 전송\n",
    " * 실제 여기서 나온 점수로 채점을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_x_text_list = []\n",
    "with open(\"soma8_eval_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        eval_x_text_list.append((info['pid'],info['name']))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list = gsvc.predict(vectorizer.transform(extract_noun(eval_x_text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"eval_prediction.pickle\", 'rb') as f:\n",
    "#     pred_list = pickle.load(f)\n",
    "# #     pred_list = \",\".join(map(lambda i : str(int(i)),pred_list.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name='정윤수'\n",
    "nickname='text_cnn인데 더 안좋네'\n",
    "mode='eval'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list.tolist())),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "\n",
    "print (d.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 으로 추출한 이미지 데이터 사용하기 \n",
    " * keras mobilenet 으로 추출한 데이터, 이 데이터를 아래처럼 읽어서 사용 가능함\n",
    " * 더 성능이 높은 모델로 이미지 피쳐를 추출하면 성능 향상 가능함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pid_img_feature_dict = {}\n",
    "with open(\"refined_category_dataset.img_feature.dat\") as fin:\n",
    "    for idx,line in enumerate(fin):\n",
    "        if idx%100 == 0:\n",
    "            print(idx)\n",
    "        pid, img_feature_str = line.strip().split(\" \")\n",
    "        img_feature = (np.asarray(list(map(lambda i : float(i),img_feature_str.split(\",\")))))\n",
    "        pid_img_feature_dict[pid] = img_feature\n",
    "#         print (line)\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_feature_list = []\n",
    "for pid, name in X_train:\n",
    "#     print(pid, name)\n",
    "    if pid in pid_img_feature_dict:\n",
    "        img_feature_list.append(pid_img_feature_dict[pid])\n",
    "#         print (len(pid_img_feature_dict[pid]),type(pid_img_feature_dict[pid]))\n",
    "#         break\n",
    "    else:\n",
    "        img_feature_list.append(np.zeros(1000))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_feature_t_list = []\n",
    "for pid, name in X_test:\n",
    "    if pid in pid_img_feature_dict:\n",
    "        img_feature_t_list.append(pid_img_feature_dict[pid])\n",
    "    else:\n",
    "        img_feature_t_list.append(np.zeros(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_feature_t_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래 부분은 text feature 와 이미지 feature 를 합쳐서 feature 를 만드는 부분이다. 이 부분에 대해서는 각자 한번 합치는 방법을 찾아 보면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_x_list = sparse.hstack((vectorizer.transform(X_train_noun), img_feature_list),format='csr')\n",
    "concat_t_x_list = sparse.hstack((vectorizer.transform(X_test_noun), img_feature_t_list), format='csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_refined_list = sparse.hstack((vectorizer.transform(X_train_noun + X_test_noun), img_feature_list + img_feature_t_list), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    c = 0.01 + (i / 300)\n",
    "    clf2 = LinearSVC(C=c)\n",
    "    clf2.fit(concat_x_list, y_train)\n",
    "    print (c,clf2.score(concat_t_x_list, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0.021\n",
    "clf2 = LinearSVC(C=c)\n",
    "clf2.fit(concat_refined_list, y_train + y_test)\n",
    "print (c,clf2.score(concat_t_x_list, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_param = np.logspace(-1 , 1, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsvc = GridSearchCV(LinearSVC(), param_grid= {'C': svc_param}, cv = 5, n_jobs = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsvc.fit(concat_refined_list, y_train + y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gsvc.best_score_, gsvc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del pid_img_feature_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 피쳐를 추가 해서 분류후 평가 서버에 분류 결과를 전송 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pid_img_feature_dict = {}\n",
    "with open(\"refined_category_dataset.img_feature.dat\") as fin:\n",
    "    for idx,line in enumerate(fin):\n",
    "        if idx%100 == 0:\n",
    "            print(idx)\n",
    "        pid, img_feature_str = line.strip().split(\" \")\n",
    "        img_feature = (np.asarray(list(map(lambda i : float(i),img_feature_str.split(\",\")))))\n",
    "        pid_img_feature_dict[pid] = img_feature\n",
    "#         print (line)\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_text_list = []\n",
    "with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        test_x_text_list.append((info['pid'],info['name']))\n",
    "\n",
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_feature_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pid, name in test_x_text_list:\n",
    "    if pid in pid_img_feature_dict:\n",
    "        img_feature_test_list.append(pid_img_feature_dict[pid])\n",
    "    else:\n",
    "        img_feature_test_list.append(np.zeros(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test_feature_list = vectorizer.transform(extract_noun(test_x_text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2개 feature 를 합치는 방법 찾아보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_test_x_list = sparse.hstack((x_test_feature_list, img_feature_test_list),format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_list = clf2.predict(concat_test_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list = gsvc.predict(concat_test_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "name='정윤수'\n",
    "nickname='brother'\n",
    "mode='test'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list.tolist())),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_x_text_list = []\n",
    "with open(\"soma8_eval_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        eval_x_text_list.append((info['pid'],info['name']))\n",
    "\n",
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_feature_eval_list = []\n",
    "for pid, name in eval_x_text_list:\n",
    "    if pid in pid_img_feature_dict:\n",
    "        img_feature_eval_list.append(pid_img_feature_dict[pid])\n",
    "    else:\n",
    "        img_feature_eval_list.append(np.zeros(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_feature_list = vectorizer.transform(final(eval_x_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_eval_x_list = sparse.hstack((x_feature_list, img_feature_eval_list),format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list = clf2.predict(concat_eval_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = gsvc.predict(concat_eval_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "name='정윤수'\n",
    "nickname='brother_svm_'\n",
    "mode='eval'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list.tolist())),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=60)\n",
    "X_train_noun = extract_noun(X_train)\n",
    "X_test_noun = extract_noun(X_test)\n",
    "\n",
    "mode_test_x_text_list = []\n",
    "with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        mode_test_x_text_list.append((info['pid'],info['name']))\n",
    "        \n",
    "mode_eval_x_text_list = []\n",
    "with open(\"soma8_eval_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        mode_eval_x_text_list.append((info['pid'],info['name']))\n",
    "\n",
    "mode_test_x_noun = extract_noun(mode_test_x_text_list)\n",
    "mode_eval_x_noun = extract_noun(mode_eval_x_text_list)\n",
    "        \n",
    "pid_img_feature_train_dict = {}\n",
    "with open(\"refined_category_dataset.img_feature.dat\") as fin:\n",
    "    for idx,line in enumerate(fin):\n",
    "        if idx%100 == 0:\n",
    "            print(idx)\n",
    "        pid, img_feature_str = line.strip().split(\" \")\n",
    "        img_feature = (np.asarray(list(map(lambda i : float(i),img_feature_str.split(\",\")))))\n",
    "        pid_img_feature_train_dict[pid] = img_feature\n",
    "\n",
    "img_feature_train_list = []\n",
    "for pid, name in X_train:\n",
    "    if pid in pid_img_feature_train_dict:\n",
    "        img_feature_train_list.append(pid_img_feature_train_dict[pid])\n",
    "    else:\n",
    "        img_feature_train_list.append(np.zeros(1000))\n",
    "\n",
    "img_feature_test_list = []\n",
    "for pid, name in X_test:\n",
    "    if pid in pid_img_feature_dict:\n",
    "        img_feature_test_list.append(pid_img_feature_dict[pid])\n",
    "    else:\n",
    "        img_feature_test_list.append(np.zeros(1000))\n",
    "\n",
    "        \n",
    "pid_img_feature_eval_dict = {}\n",
    "with open(\"refined_category_dataset.img_feature.eval.dat\") as fin:\n",
    "    for idx,line in enumerate(fin):\n",
    "        pid, img_feature_str = line.strip().split(\" \")\n",
    "        img_feature = (np.asarray(list(map(lambda i : float(i),img_feature_str.split(\",\")))))\n",
    "        pid_img_feature_eval_dict[pid] = img_feature\n",
    "#         print (line)\n",
    "#         break\n",
    "\n",
    "\n",
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "img_feature_mode_test_list = []\n",
    "for pid, name in mode_test_x_text_list:\n",
    "    if pid in pid_img_feature_eval_dict:\n",
    "        img_feature_mode_test_list.append(pid_img_feature_eval_dict[pid])\n",
    "    else:\n",
    "        img_feature_mode_test_list.append(np.zeros(1000))\n",
    "        \n",
    "        \n",
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "img_feature_eval_list = []\n",
    "for pid, name in mode_eval_x_text_list:\n",
    "    if pid in pid_img_feature_eval_dict:\n",
    "        img_feature_eval_list.append(pid_img_feature_eval_dict[pid])\n",
    "    else:\n",
    "        img_feature_eval_list.append(np.zeros(1000))\n",
    "        \n",
    "\n",
    "\n",
    "concat_x_train_list = sparse.hstack((vectorizer.transform(X_train_noun), img_feature_train_list))\n",
    "concat_x_test_list = sparse.hstack((vectorizer.transform(X_test_noun), img_feature_test_list))\n",
    "\n",
    "x_feature_eval_list = vectorizer.transform(mode_eval_x_noun)\n",
    "x_feature_test_list = vectorizer.transform(mode_test_x_noun)\n",
    "concat_x_eval_list = sparse.hstack((x_feature_eval_list, img_feature_eval_list),format='csr')\n",
    "concat_x_mode_test_list = sparse.hstack((x_feature_test_list, img_feature_mode_test_list),format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_refi_list = sparse.hstack((vectorizer.transform(X_train_noun + X_test_noun), img_feature_train_list + img_feature_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, 25):\n",
    "    c = 0.072 + (i / 1000)\n",
    "    clf = LinearSVC(C=c)\n",
    "    clf.fit(vectorizer.transform(X_train_noun), y_train)\n",
    "    print (c,clf.score(vectorizer.transform(X_test_noun), y_test))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0.094\n",
    "clf = LinearSVC(C=c)\n",
    "clf.fit(vectorizer.transform(X_train_noun), y_train)\n",
    "print (c,clf.score(vectorizer.transform(X_test_noun), y_test))\n",
    "pred_list = clf.predict(vectorizer.transform(mode_test_x_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0.094\n",
    "clf2 = LinearSVC(C=c)\n",
    "clf2.fit(concat_x_train_list, y_train)\n",
    "print (c,clf2.score(concat_x_test_list, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = clf2.predict(concat_x_mode_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list = gsvc.predict(concat_x_mode_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "name='정윤수'\n",
    "nickname='편brew5_svm'\n",
    "mode='test'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list.tolist())),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "\n",
    "print (d.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    c = 0.10 + (i / 1000)\n",
    "    clf2 = LinearSVC(C=c)\n",
    "    clf2.fit(concat_x_train_list, y_train)\n",
    "    print (c,clf2.score(concat_x_test_list, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    c = 0.10 + (i / 100)\n",
    "    clf2 = LinearSVC(C=c)\n",
    "    clf2.fit(concat_refi_list, y_train + y_test)\n",
    "    print (c,clf2.score(concat_x_test_list, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 0.30\n",
    "clf2 = LinearSVC(C=c)\n",
    "clf2.fit(concat_refi_list, y_train + y_test)\n",
    "print (c,clf2.score(concat_x_test_list, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = clf2.predict(concat_x_eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "name='정윤수'\n",
    "nickname='편brew_svm'\n",
    "mode='eval'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list.tolist())),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
